# Conclusion and Future Work 

## Summary and Discussion of Thesis Results

The work developed in this thesis challenges the existing methods used to scale a consumer group and how the load should be distributed between the various consumers in a single consumer group.

The goal was to deterministically solve the autoscaling problem related to a Kafka consumer group, which in turn was modeled as a Bin Packing Problem where the items that have to be assigned a bin can change in size over time. Due to this particular feature, a new metric was introduced to determine a rebalance score (Rscore) for a given iteration. 

The Rscore introduced a new concern when executing an algorithm to solve the bin packing problem, and for this reason 4 new algorithms were presented to heuristically solve the BPP, while at the same time taking into consideration how the items (partitions) were rebalanced to different containers (consumers). Three of the presented algorithms (MWF, MBFP and MBF), proved themselves to be a competitive alternative, consistently belonging to the pareto front when tested with the different deltas \ref{}.

To further consolidate the Bin Packing Model, the consumer was tested in diverse conditions to determine whether a constant bin size model could be used for the scenario at hand. Provided the results in \ref{}, the BPP was then modeled as its single bin size variant.

The different parts in the system interacted with one another as expected to deliver an autoscaling consumer group capable of adapting to varying loads in the system (\ref{}).

## Future Work

Given the multiple parts developed within this system, there is a lot of room for improvement. Most notably, due to the close relationship between the system and the Kubernetes cluster, the autoscaler could be wrapped as a Kubernetes Operator \ref{} as is common with systems that use Kubernetes to provide some kind of autoscaling \cite{https://www.kubegres.io/}, \cite{https://www.pulumi.com/docs/guides/continuous-delivery/pulumi-kubernetes-operator/}

### Monitor

To the best of my knowledge, for lack of a better metric being provided by the Kafka cluster, the value that was used to track the write speed of each partition was the number of bytes in each partition. For this reason, a process \ref{monitor process} was used to evaluate the speed of each partition by historically storing the amount of bytes in a partition at a certain timestamp.

A clear disadvantage of using this component is the fact that if records that are older than \lstinline[language=Python]{retention.ms} are deleted, the amount of bytes in a partition reduces, which is then erroneously evaluated as a negative speed by \ref{monitor process}. To circumvent this behaviour, either one of the following metrics provided by Kafka would suffice: The current write speed (bytes/s) per partition; A historic amouont of bytes that have been written to a partition

Since the monitor process presents the write speed as the average data rate over the last 30 seconds, using a different strategy to evaluate the speed could lead to more robust measures of the evaluated write speed of each partition.
 
### Consumer

If the network is experiencing increased latency, the current implementation of the autoscaler does not account for changes in the size of the consumer capacity, therefore the controller assumes a maximum capacity of a consumer which is not accurate to the real case. Metrics related to a consumer's performance provided by Kafka JMX metrics could be leveraged to obtain a more accurate dynamic representation of the consumer's current capacity. This could also lead to a variable bin size BPP.

### Controller

Throughout this thesis, the controller process was presented with strict execution rules and actions to manage the Kafka consumer group. Evolving this component into a more abstract concept, not only can it maintain its base functionalities, but it could also be used to more accurately solve load balancing between a consumer group without necessarily modeling the group as a BPP. 

In fact, the heuristic algorithms that make use of the worst fit rule when assigning partitions to consumers, are at the same time applying a load balancer rule between the available consumers.  

## Final Remarks

In spite of the fact that, as a whole, the thesis presents quite a specific use case, individually several of Kafka's functionalities are challenged and tested alternatives are also provided which can be used as a foundation so as to improve the way in which a consumer's load is modeled, and the manner in which the load is distributed between the elements of a consumer group.